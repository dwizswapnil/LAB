{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgmHBb-tJm43"
      },
      "outputs": [],
      "source": [
        "#1 Write a python program to perform the basic mathematical operation and matrix operations using tensor flow.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Basic Operations\n",
        "\n",
        "a = tf.constant(5)\n",
        "b = tf.constant(3)\n",
        "\n",
        "# Addition\n",
        "c = tf.add(a, b)\n",
        "print(\"Addition of a and b: \", c.numpy())\n",
        "\n",
        "# Subtraction\n",
        "d = tf.subtract(a, b)\n",
        "print(\"Subtraction of a and b: \", d.numpy())\n",
        "\n",
        "# Multiplication\n",
        "e = tf.multiply(a, b)\n",
        "print(\"Multiplication of a and b: \", e.numpy())\n",
        "\n",
        "# Division\n",
        "f = tf.divide(a, b)\n",
        "print(\"Division of a and b: \", f.numpy())\n",
        "\n",
        "# Matrix Operations\n",
        "\n",
        "# Matrix 1\n",
        "matrix1 = tf.constant([[1, 2], [3, 4]])\n",
        "print(\"Matrix 1: \\n\", matrix1.numpy())\n",
        "\n",
        "# Matrix 2\n",
        "matrix2 = tf.constant([[5, 6], [7, 8]])\n",
        "print(\"Matrix 2: \\n\", matrix2.numpy())\n",
        "\n",
        "# Matrix Addition\n",
        "matrix_add = tf.add(matrix1, matrix2)\n",
        "print(\"Matrix Addition: \\n\", matrix_add.numpy())\n",
        "\n",
        "# Matrix Subtraction\n",
        "matrix_sub = tf.subtract(matrix1, matrix2)\n",
        "print(\"Matrix Subtraction: \\n\", matrix_sub.numpy())\n",
        "\n",
        "# Matrix Multiplication\n",
        "matrix_mul = tf.matmul(matrix1, matrix2)\n",
        "print(\"Matrix Multiplication: \\n\", matrix_mul.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMBaRomSr8SN",
        "outputId": "bc5fe367-8539-4a8e-e974-77778627a93c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Write a python program to perform the basic logic gates AND , OR using Mcculloch pitts model.\n",
        "\n",
        "class McCullochPitts:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def activate(self, inputs):\n",
        "        weighted_sum = sum([w * x for w, x in zip(self.weights, inputs)])\n",
        "        return 1 if weighted_sum + self.bias > 0 else 0\n",
        "\n",
        "class ANDGate(McCullochPitts):\n",
        "    def __init__(self):\n",
        "        super().__init__([1, 1], -1)\n",
        "\n",
        "class ORGate(McCullochPitts):\n",
        "    def __init__(self):\n",
        "        super().__init__([1, 1], 0)\n",
        "\n",
        "# Example usage:\n",
        "and_gate = ANDGate()\n",
        "or_gate = ORGate()\n",
        "\n",
        "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "\n",
        "print(\"AND gate:\")\n",
        "for i in inputs:\n",
        "    output = and_gate.activate(i)\n",
        "    print(f\"{i[0]} AND {i[1]} = {output}\")\n",
        "\n",
        "print(\"\\nOR gate:\")\n",
        "for i in inputs:\n",
        "    output = or_gate.activate(i)\n",
        "    print(f\"{i[0]} OR {i[1]} = {output}\")\n"
      ],
      "metadata": {
        "id": "sm8OX1j0JwqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Write a python program to implement NAND and NOR gate using Mcculloch pitts model.\n",
        "\n",
        "class McCullochPitts:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def activate(self, inputs):\n",
        "        weighted_sum = sum([w * x for w, x in zip(self.weights, inputs)])\n",
        "        return 1 if weighted_sum + self.bias > 0 else 0\n",
        "\n",
        "class NANDGate(McCullochPitts):\n",
        "    def __init__(self):\n",
        "        super().__init__([-1, -1], 2)\n",
        "\n",
        "class NORGate(McCullochPitts):\n",
        "    def __init__(self):\n",
        "        super().__init__([-1, -1], 1)\n",
        "\n",
        "# Example usage:\n",
        "nand_gate = NANDGate()\n",
        "nor_gate = NORGate()\n",
        "\n",
        "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "\n",
        "print(\"NAND gate:\")\n",
        "for i in inputs:\n",
        "    output = nand_gate.activate(i)\n",
        "    print(f\"{i[0]} NAND {i[1]} = {output}\")\n",
        "\n",
        "print(\"\\nNOR gate:\")\n",
        "for i in inputs:\n",
        "    output = nor_gate.activate(i)\n",
        "    print(f\"{i[0]} NOR {i[1]} = {output}\")\n"
      ],
      "metadata": {
        "id": "mvPH8fmOKoC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Write a python program to implement logistic regression using house price data set\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Boston house price dataset\n",
        "df = pd.read_csv('/content/housing.csv', header=0)\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Convert the target variable to binary\n",
        "y = y.astype(float)\n",
        "y = np.where(y >= np.median(y), 1, 0)\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel(\"True Values\")\n",
        "plt.ylabel(\"Predictions\")\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy*100)\n",
        "\n",
        "\n",
        "# Create a new dataframe with predicted probabilities and true target values\n",
        "output_df = pd.DataFrame({'Predicted Probability': y_pred, 'True Target Values': y_test})\n",
        "\n",
        "# Print the output dataframe\n",
        "print(output_df.head())\n"
      ],
      "metadata": {
        "id": "4Gqa7Y-7Lyx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Write a python program to implement MLP classifier using digit classification data set.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the digits dataset\n",
        "digits = load_digits()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the MLP classifier with 2 hidden layers of 16 neurons each\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(16, 16), max_iter=1000)\n",
        "\n",
        "# Train the classifier on the training set\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy %:\", accuracy*100)\n",
        "\n",
        "# Plot some example images and their predicted labels\n",
        "fig, ax = plt.subplots(4, 4, figsize=(8, 8))\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(X_test[i].reshape(8, 8), cmap='gray')\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.set_ylabel(f\"Predicted: {y_pred[i]}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "krFSUXVzMgoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Write a python program to implement linear binary classifier using multilayer perceptron.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
        "df = pd.read_csv(url, header=None)\n",
        "\n",
        "# Add column names\n",
        "columns = ['id', 'diagnosis', 'mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area',\n",
        "           'mean_smoothness', 'mean_compactness', 'mean_concavity', 'mean_concave_points',\n",
        "           'mean_symmetry', 'mean_fractal_dimension', 'radius_se', 'texture_se', 'perimeter_se',\n",
        "           'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave_points_se',\n",
        "           'symmetry_se', 'fractal_dimension_se', 'worst_radius', 'worst_texture', 'worst_perimeter',\n",
        "           'worst_area', 'worst_smoothness', 'worst_compactness', 'worst_concavity', 'worst_concave_points',\n",
        "           'worst_symmetry', 'worst_fractal_dimension']\n",
        "df.columns = columns\n",
        "\n",
        "# Drop the id column as it is not useful for classification\n",
        "df.drop('id', axis=1, inplace=True)\n",
        "\n",
        "# Encode the diagnosis column using one-hot encoding\n",
        "diagnosis_encoded = pd.get_dummies(df['diagnosis'])\n",
        "df.drop('diagnosis', axis=1, inplace=True)\n",
        "encoded_df = pd.concat([df, diagnosis_encoded], axis=1)\n",
        "\n",
        "X = encoded_df.iloc[:, :-2].values\n",
        "y = encoded_df.iloc[:, -2:].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the MLP classifier\n",
        "model = MLPClassifier(hidden_layer_sizes=(1,), activation='identity', solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Concatenate the predicted values with the test set\n",
        "predicted_df = pd.DataFrame(y_pred, columns=['predicted_B', 'predicted_M'])\n",
        "test_df = pd.DataFrame(y_test, columns=['actual_B', 'actual_M'])\n",
        "result_df = pd.concat([test_df, predicted_df], axis=1)\n",
        "\n",
        "# Print the resulting dataframe\n",
        "print(result_df.head())\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy %:\", accuracy*100)\n"
      ],
      "metadata": {
        "id": "xiKgg37FTCGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 Write a python program in which the database contains 76 attributes but all published experiments refer to using a subset of 14 of them the target field in it is integer value 0 is equal to no, and 1 is equal to more chance of heart attack for performing classification (data set is available on kaggle).\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/heart.csv\")\n",
        "\n",
        "# Select a subset of 14 features\n",
        "feature_cols = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
        "                'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
        "X = df[feature_cols].values\n",
        "\n",
        "# Perform binary classification on the target field\n",
        "y = df['target'].apply(lambda x: 1 if x > 0 else 0).values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the MLP classifier\n",
        "model = MLPClassifier(hidden_layer_sizes=(5,), activation='relu', solver='adam')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy %:\", accuracy*100)\n",
        "\n",
        "# Print the output dataset\n",
        "output_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "test_df = pd.DataFrame(X_test, columns=feature_cols)\n",
        "output_df = pd.concat([test_df, output_df], axis=1)\n",
        "print(output_df.head())"
      ],
      "metadata": {
        "id": "N1p18BWgTVhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Implement ann using keras lib for binary classification\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import regularizers\n",
        "\n",
        "# Define the input data and labels\n",
        "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_train = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=2, activation='tanh', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5000, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_test = np.array([0, 1, 1, 0])\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n"
      ],
      "metadata": {
        "id": "GEkHz9muP-1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a3b950-1cd8-4f94-8eb1-8603041aa445"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9\n",
        "# In the given dataset there are various factors which are involved when the patient is hospitalised on the basis of these factors predicting whether the patient will survive or not. The dataset has 85 columns so perform reduction using PCA and normalise the data and perform classification using ANN (patient data set is available on kaggle)\n",
        "\n",
        "# dataset link  https://www.kaggle.com/andrewmvd/heart-failure-clinical-data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('heart_failure_clinical_records_dataset.csv')\n",
        "\n",
        "# Split the data into features and labels\n",
        "X = data.drop('DEATH_EVENT', axis=1)\n",
        "y = data['DEATH_EVENT']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_norm = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Accuracy:2f' % (accuracy*100))"
      ],
      "metadata": {
        "id": "_yXngKM5wwtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10\n",
        "# Write a program to Build a prediction model that will perform the following:\n",
        "# a) classified if a customer is going to churn or not.\n",
        "# b) preferably based on the model performance to improve the accuracy.\n",
        "\n",
        "# # create a csv ile with this data and import it in the model\n",
        "#customer_id,age,gender,income,credit_score,num_of_products,has_churned\n",
        "# 1,25,M,50000,600,2,0\n",
        "# 2,35,F,75000,700,1,0\n",
        "# 3,45,M,100000,800,3,0\n",
        "# 4,30,F,60000,650,2,1\n",
        "# 5,50,M,120000,750,1,0\n",
        "# 6,40,F,90000,700,2,1\n",
        "# ,55,M,150000,800,3,0\n",
        "# 8,28,F,55000,600,1,0\n",
        "# 9,32,M,65000,700,2,0\n",
        "# 10,48,F,110000,750,3,0\n",
        "\n",
        "# import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# load dataset\n",
        "df = pd.read_csv(\"customer_churn_dataset.csv\")\n",
        "\n",
        "# create dummy variables for categorical features\n",
        "# df = pd.get_dummies(df, columns=[\"gender\", \"region\", \"partner\", \"dependents\", \"phone_service\", \"multiple_lines\", \"internet_service\", \"online_security\", \"online_backup\", \"device_protection\", \"tech_support\", \"streaming_tv\", \"streaming_movies\", \"contract\", \"paperless_billing\", \"payment_method\"])\n",
        "\n",
        "# split dataset into training and testing sets\n",
        "X = df.drop(\"churn\", axis=1)\n",
        "y = df[\"churn\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# create logistic regression model\n",
        "lr = LogisticRegression(random_state=42)\n",
        "\n",
        "# define hyperparameters for grid search\n",
        "params = {\"penalty\": [\"l1\", \"l2\"], \"C\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# perform grid search to find best hyperparameters\n",
        "gs = GridSearchCV(lr, params, cv=5)\n",
        "gs.fit(X_train, y_train)\n",
        "\n",
        "# evaluate model on test set\n",
        "y_pred = gs.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "8BhMOU1OX9XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11\n",
        "# Write a python program to build a weather predicton model for the dates given also Visualize the actual predicted data using matplotlib.\n",
        "# https://www.kaggle.com/muthuj7/weather-dataset\n",
        "\n",
        "# import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# load dataset\n",
        "df = pd.read_csv(\"/content/sample_data/weatherHistory.csv\")\n",
        "\n",
        "# extract features and target variable\n",
        "X = df[[\"Humidity\", \"Wind Speed (km/h)\"]].values\n",
        "y = df[\"Temperature (C)\"].values\n",
        "\n",
        "# split dataset into training and testing sets\n",
        "split = int(0.8*len(df))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# create linear regression model\n",
        "lr = LinearRegression()\n",
        "\n",
        "# train model on training set\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# predict temperature for test set\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "# calculate metrics for evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R2 Score:\", r2)\n",
        "\n",
        "# plot actual and predicted temperature values\n",
        "plt.plot(y_test, label=\"Actual Temperature\")\n",
        "plt.plot(y_pred, label=\"Predicted Temperature\")\n",
        "plt.xlabel(\"Time (Days)\")\n",
        "plt.ylabel(\"Temperature (C)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9Be_iwPWMORY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12\n",
        "# Normalise the data in the given dataset in question 11, and perform classification using ANN.\n",
        "# https://www.kaggle.com/muthuj7/weather-dataset\n",
        "# import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# load dataset\n",
        "df = pd.read_csv(\"/content/sample_data/weatherHistory.csv\")\n",
        "\n",
        "# extract features and target variable\n",
        "X = df[[\"Humidity\", \"Wind Speed (km/h)\"]].values\n",
        "y = df[\"Temperature (C)\"].values\n",
        "\n",
        "# normalize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# create ANN model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=2, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
        "\n",
        "# train model on training set\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# evaluate model on test set\n",
        "loss, mse = model.evaluate(X_test, y_test)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# plot training and validation loss over epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "voLMqXDgM9YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13\n",
        "# Using Neural network train the model with train dataset , predict the covid  death cases  and confirmed cases. visualize the actual predict data using matplotlib. use20days time stamp.\n",
        "# url for data set download https://www.kaggle.com/datasets/imdevskp/corona-virus-report?select=full_grouped.csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "\n",
        "# Load data from CSV file\n",
        "df = pd.read_csv('/content/full_grouped.csv')\n",
        "\n",
        "# Extract the relevant features\n",
        "data = df[['Confirmed', 'Deaths']].values.astype(float)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(data) * 0.8)\n",
        "train_data = data[:train_size, :]\n",
        "test_data = data[train_size:, :]\n",
        "\n",
        "# Define the time stamp\n",
        "time_stamp = 20\n",
        "\n",
        "# Define the input and target variables for training and testing\n",
        "x_train, y_train = [], []\n",
        "x_test, y_test = [], []\n",
        "\n",
        "for i in range(time_stamp, len(train_data)):\n",
        "    x_train.append(train_data[i-time_stamp:i, :])\n",
        "    y_train.append(train_data[i, :])\n",
        "\n",
        "for i in range(time_stamp, len(test_data)):\n",
        "    x_test.append(test_data[i-time_stamp:i, :])\n",
        "    y_test.append(test_data[i, :])\n",
        "\n",
        "# Convert the input and target variables to numpy arrays\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "x_test, y_test = np.array(x_test), np.array(y_test)\n",
        "\n",
        "# Define the neural network model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(time_stamp, 2)))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(2))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "# Predict the number of confirmed cases and deaths for the next 20 days\n",
        "x_pred = data[-time_stamp:, :]\n",
        "x_pred = np.reshape(x_pred, (1, time_stamp, 2))\n",
        "y_pred = model.predict(x_pred)\n",
        "y_pred = scaler.inverse_transform(y_pred)\n",
        "y_pred = y_pred.reshape((2,)) # reshape y_pred from (1, 2) to (2,)\n",
        "# Visualize the actual and predicted data using matplotlib\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "ax.plot(df['Confirmed'], label='Actual Confirmed')\n",
        "ax.plot(df.index[-2:], y_pred[0:], label='Predicted Confirmed') # use y_pred[0] for confirmed cases\n",
        "ax.plot(df['Deaths'], label='Actual Deaths')\n",
        "ax.plot(df.index[-1:], y_pred[1:], label='Predicted Deaths') # use y_pred[1] for death cases\n",
        "\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Number of Cases')\n",
        "ax.set_title('COVID-19 Prediction')\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dkLI_b-qQVQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14\n",
        "# a.\tBuild CNN for identifying gestures of human being.\n",
        "# b.\tImprove the model tuning hyper parameters \n",
        "# Url for data set download https://www.kaggle.com/datasets/grassknoted/asl-alphabet?select=asl_alphabet_test\n",
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# define paths to the dataset\n",
        "train_path = 'asl_alphabet_train'\n",
        "test_path = 'asl_alphabet_test'\n",
        "\n",
        "# define the data generators for train and test sets\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, \n",
        "                                   rotation_range=20, \n",
        "                                   width_shift_range=0.2, \n",
        "                                   height_shift_range=0.2, \n",
        "                                   shear_range=0.2, \n",
        "                                   zoom_range=0.2, \n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_path, \n",
        "                                                    target_size=(64, 64), \n",
        "                                                    batch_size=32, \n",
        "                                                    class_mode='categorical')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(test_path, \n",
        "                                                  target_size=(64, 64), \n",
        "                                                  batch_size=32, \n",
        "                                                  class_mode='categorical')\n",
        "\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# define the CNN model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(29, activation='softmax'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# train the model\n",
        "model.fit(train_generator, epochs=10, validation_data=test_generator)\n",
        "\n",
        "# evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f'Test loss: {test_loss:.4f}')\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# define the function that creates the model\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model\n"
      ],
      "metadata": {
        "id": "qQIeYJnUQVXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15\n",
        "# Build a deep learning model which classifies cats and dogs using CNN.\n",
        "\n",
        "# URL of the dataset for download\n",
        "# url = 'https://www.microsoft.com/en-us/download/details.aspx?id=54765'\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,shear_range=0.2,\n",
        "    zoom_range=0.2,horizontal_flip=True,validation_split=0.2)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/cats_and_dogs_filtered/train',target_size=(224, 224),\n",
        "    batch_size=32,class_mode='binary',subset='training')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    '/content/cats_and_dogs_filtered/train',target_size=(224, 224),\n",
        "    batch_size=32,class_mode='binary',subset='validation')\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
        "    layers.MaxPooling2D((2,2)),layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),layers.Flatten(),layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator,epochs=10,validation_data=validation_generator,verbose=1)\n",
        "test_generator = train_datagen.flow_from_directory('./cats-vs-dogs/test',\n",
        "                                                   target_size=(224, 224),batch_size=32,class_mode='binary')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "Ebjb70xOV-cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16\n",
        "# The stock price of the present day can be predicted by the stock prices obtained for past 50 days. Using Simple RNN train the model with “Google_stock_price_train.csv” dataset, predict the stock prices for the dates given in “Google_stock_price_train.csv” dataset and visualize the actual  predicted prices using matplotlib\n",
        "# url for dataset download https://www.kaggle.com/datasets/vaibhavsxn/google-stock-prices-training-and-test-data?select=Google_Stock_Price_Train.csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Dropout\n",
        "\n",
        "# load the data\n",
        "df = pd.read_csv('/content/sample_data/Google_Stock_Price_Train.csv')\n",
        "\n",
        "# preprocess the data\n",
        "df['Close'] = df['Close'].str.replace(',', '').astype(float)\n",
        "training_data = df['Close'].values.reshape(-1, 1)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "training_data = scaler.fit_transform(training_data)\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "# prepare the data for RNN\n",
        "for i in range(50, len(training_data)):\n",
        "    X_train.append(training_data[i-50:i, 0])\n",
        "    y_train.append(training_data[i, 0])\n",
        "\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "# build the model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(SimpleRNN(units=50))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "\n",
        "# predict on the training data\n",
        "y_pred = model.predict(X_train)\n",
        "\n",
        "# inverse transform the predicted and actual data\n",
        "y_pred = scaler.inverse_transform(y_pred)\n",
        "y_train = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
        "\n",
        "# visualize the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(y_train, label='Actual')\n",
        "plt.plot(y_pred, label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0JPUYfgnWdVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17\n",
        "# Write a python program to Use a stochastic gradient descent optimizer for the similar classification problem and compare the result of both the model in the first case you can use optimizer Adam.\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encode the target variable\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "# Compile the model using Adam optimizer\n",
        "adam_optimizer = Adam()\n",
        "model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "adam_history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
        "\n",
        "# Compile the model using SGD optimizer\n",
        "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "sgd_history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
        "\n",
        "# Compare the results\n",
        "print(\"Adam optimizer accuracy: \", np.mean(adam_history.history['val_accuracy']))\n",
        "print(\"SGD optimizer accuracy: \", np.mean(sgd_history.history['val_accuracy']))\n"
      ],
      "metadata": {
        "id": "JooSRlnEaOLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a python program to perform Multiclass classification on MNIST data set by Artificial neural network use softmax activation function for the output layer Evaluate the performance by using confusion matrix.\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Define the ANN model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Create a confusion matrix\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "ax.imshow(cm, cmap='Blues')\n",
        "ax.grid(False)\n",
        "ax.set_xlabel('Predicted labels', fontsize=12, color='black')\n",
        "ax.set_ylabel('True labels', fontsize=12, color='black')\n",
        "ax.set_xticks(range(10))\n",
        "ax.set_yticks(range(10))\n",
        "ax.xaxis.set_ticklabels(range(10), fontsize=10)\n",
        "ax.yaxis.set_ticklabels(range(10), fontsize=10)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZyaaRRUsbUsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19\n",
        "# Perform Multi class classification on MNIST dataset using ANN. (Note: Use softmax activation function for the output layer with 3-4 hidden layers consisting of ReLU activation function also evaluate the performance by using confusion matrix.)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Reshape input data to a flat vector of 784 pixels\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "# Convert target variable to categorical format\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Define model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=128,\n",
        "                    epochs=20,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.imshow(cm, cmap='binary')\n",
        "plt.colorbar()\n",
        "plt.xticks(np.arange(10))\n",
        "plt.yticks(np.arange(10))\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XVFJCRd3bjJN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}